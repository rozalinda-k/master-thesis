{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_fscore_support\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from catboost.utils import eval_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOTAB CTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotab_cta_train = pd.read_csv('SOTAB CTA/sotab_v2_cta_training_set.csv')\n",
    "sotab_cta_test = pd.read_csv('SOTAB CTA/sample_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotab_cta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_table=pd.DataFrame()\n",
    "table=sotab_cta_test\n",
    "primary_path='SOTAB CTA/Test/'\n",
    "\n",
    "\n",
    "all_data_from_all_columns = []\n",
    "all_target = []\n",
    "for table_index, table_values in table.iterrows():\n",
    "\n",
    "    full_path = primary_path + table_values[0]\n",
    "    index_of_desired_columns = table_values[1]\n",
    "    label_of_table = table_values[2]\n",
    "\n",
    "    df = pd.read_json(full_path, compression='gzip', lines=True) \n",
    "\n",
    "    all_values_from_desired_columns = []\n",
    "\n",
    "    for value in df[index_of_desired_columns]:\n",
    "        if value != None:\n",
    "            all_values_from_desired_columns.append(value)\n",
    "\n",
    "    joined_values = ', '.join(map(str, all_values_from_desired_columns))\n",
    "\n",
    "    all_data_from_all_columns.append(joined_values)\n",
    "\n",
    "    all_target.append(label_of_table)\n",
    "\n",
    "final_test_table['data']=all_data_from_all_columns\n",
    "final_test_table['target']=all_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_table=pd.DataFrame()\n",
    "table=sotab_cta_train\n",
    "primary_path='SOTAB CTA/Train/'\n",
    "\n",
    "\n",
    "all_data_from_all_columns = []\n",
    "all_target = []\n",
    "for table_index,table_values in table.iterrows():\n",
    "\n",
    "    full_path = primary_path + table_values[0]\n",
    "    index_of_desired_columns = table_values[1]\n",
    "    label_of_table = table_values[2]\n",
    "\n",
    "\n",
    "    df = pd.read_json(full_path, compression='gzip', lines=True) \n",
    "    all_values_from_desired_columns = []\n",
    "\n",
    "    for value in df[index_of_desired_columns]:\n",
    "        if value != None:\n",
    "            all_values_from_desired_columns.append(value)\n",
    "\n",
    "    joined_values = ', '.join(map(str, all_values_from_desired_columns))\n",
    "\n",
    "    all_data_from_all_columns.append(joined_values)        \n",
    "    all_target.append(label_of_table)\n",
    "\n",
    "final_train_table['data']=all_data_from_all_columns\n",
    "final_train_table['target']=all_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-10, 2016-04-08, 2013-09-13, 2016-08-05...</td>\n",
       "      <td>Date</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Sleep Revolution, Viva, Ame, Lidere, The S...</td>\n",
       "      <td>Book/name</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en-US, pt-BR, pt-BR, pt-BR, pt-BR, pt-BR, pt-B...</td>\n",
       "      <td>Language</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homer William Bedell Stanford W. Stanford, Dav...</td>\n",
       "      <td>Person/name</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paperback, Hardcover, Paperback, None, Paperba...</td>\n",
       "      <td>BookFormatType</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116882</th>\n",
       "      <td>Billions, Bonanza, Voltron: Legendary Defender...</td>\n",
       "      <td>CreativeWorkSeries</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116883</th>\n",
       "      <td>Family Guy Season 2 Episode 3, Naked and Afrai...</td>\n",
       "      <td>TVEpisode/name</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116884</th>\n",
       "      <td>Family Guy, Naked and Afraid XL, Animal Kingdo...</td>\n",
       "      <td>CreativeWorkSeries</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116885</th>\n",
       "      <td>Psych Season 2 Episode 8, Still a Mystery Seas...</td>\n",
       "      <td>TVEpisode/name</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116886</th>\n",
       "      <td>Psych, Still a Mystery, Bosch, Condor, The Ali...</td>\n",
       "      <td>CreativeWorkSeries</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116887 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     data              target  \\\n",
       "0       2020-07-10, 2016-04-08, 2013-09-13, 2016-08-05...                Date   \n",
       "1       The Sleep Revolution, Viva, Ame, Lidere, The S...           Book/name   \n",
       "2       en-US, pt-BR, pt-BR, pt-BR, pt-BR, pt-BR, pt-B...            Language   \n",
       "3       Homer William Bedell Stanford W. Stanford, Dav...         Person/name   \n",
       "4       Paperback, Hardcover, Paperback, None, Paperba...      BookFormatType   \n",
       "...                                                   ...                 ...   \n",
       "116882  Billions, Bonanza, Voltron: Legendary Defender...  CreativeWorkSeries   \n",
       "116883  Family Guy Season 2 Episode 3, Naked and Afrai...      TVEpisode/name   \n",
       "116884  Family Guy, Naked and Afraid XL, Animal Kingdo...  CreativeWorkSeries   \n",
       "116885  Psych Season 2 Episode 8, Still a Mystery Seas...      TVEpisode/name   \n",
       "116886  Psych, Still a Mystery, Bosch, Condor, The Ali...  CreativeWorkSeries   \n",
       "\n",
       "        encoded_label  \n",
       "0                   0  \n",
       "1                   1  \n",
       "2                   2  \n",
       "3                   3  \n",
       "4                   4  \n",
       "...               ...  \n",
       "116882             81  \n",
       "116883             80  \n",
       "116884             81  \n",
       "116885             80  \n",
       "116886             81  \n",
       "\n",
       "[116887 rows x 3 columns]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = list(final_train_table[\"target\"].unique())\n",
    " \n",
    "final_train_table['encoded_label'] = final_train_table['target'].apply(lambda row: types.index(row))\n",
    "final_test_table['encoded_label'] = final_test_table['target'].apply(lambda row: types.index(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_test_table.encoded_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-10, 2016-04-08, 2013-09-13, 2016-08-05...</td>\n",
       "      <td>Date</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Sleep Revolution, Viva, Ame, Lidere, The S...</td>\n",
       "      <td>Book/name</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en-US, pt-BR, pt-BR, pt-BR, pt-BR, pt-BR, pt-B...</td>\n",
       "      <td>Language</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homer William Bedell Stanford W. Stanford, Dav...</td>\n",
       "      <td>Person/name</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paperback, Hardcover, Paperback, None, Paperba...</td>\n",
       "      <td>BookFormatType</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116882</th>\n",
       "      <td>Billions, Bonanza, Voltron: Legendary Defender...</td>\n",
       "      <td>CreativeWorkSeries</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116883</th>\n",
       "      <td>Family Guy Season 2 Episode 3, Naked and Afrai...</td>\n",
       "      <td>TVEpisode/name</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116884</th>\n",
       "      <td>Family Guy, Naked and Afraid XL, Animal Kingdo...</td>\n",
       "      <td>CreativeWorkSeries</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116885</th>\n",
       "      <td>Psych Season 2 Episode 8, Still a Mystery Seas...</td>\n",
       "      <td>TVEpisode/name</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116886</th>\n",
       "      <td>Psych, Still a Mystery, Bosch, Condor, The Ali...</td>\n",
       "      <td>CreativeWorkSeries</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116887 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     data              target  \\\n",
       "0       2020-07-10, 2016-04-08, 2013-09-13, 2016-08-05...                Date   \n",
       "1       The Sleep Revolution, Viva, Ame, Lidere, The S...           Book/name   \n",
       "2       en-US, pt-BR, pt-BR, pt-BR, pt-BR, pt-BR, pt-B...            Language   \n",
       "3       Homer William Bedell Stanford W. Stanford, Dav...         Person/name   \n",
       "4       Paperback, Hardcover, Paperback, None, Paperba...      BookFormatType   \n",
       "...                                                   ...                 ...   \n",
       "116882  Billions, Bonanza, Voltron: Legendary Defender...  CreativeWorkSeries   \n",
       "116883  Family Guy Season 2 Episode 3, Naked and Afrai...      TVEpisode/name   \n",
       "116884  Family Guy, Naked and Afraid XL, Animal Kingdo...  CreativeWorkSeries   \n",
       "116885  Psych Season 2 Episode 8, Still a Mystery Seas...      TVEpisode/name   \n",
       "116886  Psych, Still a Mystery, Bosch, Condor, The Ali...  CreativeWorkSeries   \n",
       "\n",
       "        encoded_label  \n",
       "0                   0  \n",
       "1                   1  \n",
       "2                   2  \n",
       "3                   3  \n",
       "4                   4  \n",
       "...               ...  \n",
       "116882             81  \n",
       "116883             80  \n",
       "116884             81  \n",
       "116885             80  \n",
       "116886             81  \n",
       "\n",
       "[116887 rows x 3 columns]"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 344692)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3)\n",
    "X_train = tfidf.fit_transform(final_train_table['data'])\n",
    "X_test = tfidf.transform(final_test_table['data'])\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(final_train_table['encoded_label'])\n",
    "n_values = np.max(values) + 1\n",
    "y_train = np.eye(n_values)[values]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 82)"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = list(final_test_table['encoded_label'])\n",
    "y_test = np.eye(n_values)[values]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "chi_square = SelectKBest(chi2, k=1000)\n",
    "X_train = chi_square.fit_transform(X_train, y_train)\n",
    "X_test = chi_square.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = chi_square.get_feature_names_out()\n",
    "print(\"Number of features:\", len(feature_names))\n",
    "print(\"Some example features:\", feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    loss_function='MultiLogloss',\n",
    "    learning_rate=0.2,\n",
    "    iterations=100\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "precision, recall, macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_f1 = f1_score(y_test, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CatBoost Evaluation model/sotab-CTA-model2.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOTAB CPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotab_cpa_train = pd.read_csv('SOTAB CPA/sotab_v2_cpa_training_set.csv')\n",
    "sotab_cpa_test = pd.read_csv('SOTAB CPA/sample_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>main_column_index</th>\n",
       "      <th>column_index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Book_antipodean.com_September2020_CPA.json.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>publisher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Event_bfodurham.net_September2020_CPA.json.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>eventStatus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Event_bfodurham.net_September2020_CPA.json.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>eventAttendanceMode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Event_healthychelsea.org_September2020_CPA.jso...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>organizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Event_healthychelsea.org_September2020_CPA.jso...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Product_cit.li_September2020_CPA.json.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Product_cit.li_September2020_CPA.json.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>productID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Product_coininvest.com_September2020_CPA.json.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>manufacturer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Recipe_bakedbyanintrovert.com_September2020_CP...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>recipeInstructions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Recipe_confessionsofafitfoodie.com_September20...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            table_name  main_column_index  \\\n",
       "0        Book_antipodean.com_September2020_CPA.json.gz                  0   \n",
       "1        Event_bfodurham.net_September2020_CPA.json.gz                  0   \n",
       "2        Event_bfodurham.net_September2020_CPA.json.gz                  0   \n",
       "3    Event_healthychelsea.org_September2020_CPA.jso...                  0   \n",
       "4    Event_healthychelsea.org_September2020_CPA.jso...                  0   \n",
       "..                                                 ...                ...   \n",
       "504           Product_cit.li_September2020_CPA.json.gz                  0   \n",
       "505           Product_cit.li_September2020_CPA.json.gz                  0   \n",
       "506   Product_coininvest.com_September2020_CPA.json.gz                  0   \n",
       "507  Recipe_bakedbyanintrovert.com_September2020_CP...                  0   \n",
       "508  Recipe_confessionsofafitfoodie.com_September20...                  0   \n",
       "\n",
       "     column_index                label  \n",
       "0               4            publisher  \n",
       "1               3          eventStatus  \n",
       "2               4  eventAttendanceMode  \n",
       "3               4            organizer  \n",
       "4               5            telephone  \n",
       "..            ...                  ...  \n",
       "504             2               weight  \n",
       "505             4            productID  \n",
       "506             4         manufacturer  \n",
       "507            11   recipeInstructions  \n",
       "508            17               review  \n",
       "\n",
       "[509 rows x 4 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotab_cpa_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "gt = {'train':{}, 'test':{}}\n",
    "for index, row in sotab_cpa_train.iterrows():\n",
    "    if row[\"table_name\"] not in gt['train']:\n",
    "        gt['train'][row[\"table_name\"]] = {}\n",
    "    gt['train'][row[\"table_name\"]][row[\"column_index\"]] = row[\"label\"]\n",
    "    \n",
    "for index, row in sotab_cpa_test.iterrows():\n",
    "    if row[\"table_name\"] not in gt['test']:\n",
    "        gt['test'][row[\"table_name\"]] = {}\n",
    "    gt['test'][row[\"table_name\"]][row[\"column_index\"]] = row[\"label\"]\n",
    "    \n",
    "def clean_text(text):\n",
    "        \n",
    "    if(isinstance(text, dict)):\n",
    "        text = ' '.join([ clean_text(v) for k, v in text.items()] )\n",
    "    elif(isinstance(text, list)):\n",
    "        text = map(clean_text, text)\n",
    "        text = ' '.join(text)\n",
    "        \n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "\n",
    "    text = re.sub(' +', ' ', str(text)).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def get_all_table_columns(file_name, index):\n",
    "    \n",
    "\n",
    "    if file_name in sotab_cpa_train[\"table_name\"].tolist():\n",
    "        path = 'SOTAB CPA/Train/'+file_name\n",
    "    else:\n",
    "        path = 'SOTAB CPA/Test/'+file_name\n",
    "    \n",
    "    df = pd.read_json(path, compression='gzip', lines=True)\n",
    "        \n",
    "    cleaned_rows = []\n",
    "    \n",
    "\n",
    "    cleaned_main=\"{}{}\"\n",
    "\n",
    "    x = \" \".join([\" \".join(clean_text(row).split()[:20]) for row in df[0].tolist()[:5]]) \n",
    "    cleaned_main=cleaned_main.format(\"\", x)\n",
    "    cleaned_rows.append(cleaned_main)\n",
    "\n",
    "    cleaned_rows.append(\"\")\n",
    "    \n",
    "    \n",
    "    for row in df.iloc[:, index].tolist():\n",
    "        get_values_var=7\n",
    "        cleaned = \" \".join(clean_text(row).split()[:20])\n",
    "        \n",
    "        if cleaned != \"\":\n",
    "            cleaned_rows.append(cleaned)\n",
    "        else:\n",
    "            get_values_var=get_values_var-1\n",
    "\n",
    "    \n",
    "    return \" \".join(cleaned_rows[:get_values_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = []\n",
    "for table in gt['test']:\n",
    "    for column in gt['test'][table]:\n",
    "        col_str = get_all_table_columns(table, column)\n",
    "        test_examples.append([table, column, col_str, gt['test'][table][column], table.split(\"_\")[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table_sotab_cpa=pd.DataFrame()\n",
    "labels=[]\n",
    "values=[]\n",
    "for i in test_examples:\n",
    "    labels.append(i[-2])\n",
    "    values.append(i[2])\n",
    "\n",
    "final_table_sotab_cpa['data']=values\n",
    "final_table_sotab_cpa['cpa label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_sotab_cpa=final_table_sotab_cpa\n",
    "\n",
    "test_table_sotab_cpa=test_table_sotab_cpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "for table in gt['train']:\n",
    "    for column in gt['train'][table]:\n",
    "        col_str = get_all_table_columns(table, column)\n",
    "        train_examples.append([table, column, col_str, gt['train'][table][column], table.split(\"_\")[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table_sotab_cpa=pd.DataFrame()\n",
    "labels=[]\n",
    "values=[]\n",
    "for i in train_examples:\n",
    "    labels.append(i[-2])\n",
    "    values.append(i[2])\n",
    "\n",
    "final_table_sotab_cpa['data']=values\n",
    "final_table_sotab_cpa['cpa label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table_sotab_cpa=final_table_sotab_cpa\n",
    "\n",
    "train_table_sotab_cpa=train_table_sotab_cpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3)\n",
    "X_train_cpa_sotab = tfidf.fit_transform(train_table_sotab_cpa['data']) \n",
    "X_test_cpa_sotab = tfidf.transform(test_table_sotab_cpa['data'])\n",
    "\n",
    "X_test_cpa_sotab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = list(train_table_sotab_cpa[\"cpa label\"].unique())\n",
    " \n",
    "train_table_sotab_cpa['encoded_label'] = train_table_sotab_cpa['cpa label'].apply(lambda row: types.index(row))\n",
    "test_table_sotab_cpa['encoded_label'] = test_table_sotab_cpa['cpa label'].apply(lambda row: types.index(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(train_table_sotab_cpa['encoded_label'])\n",
    "n_values = np.max(values) + 1\n",
    "y_train_cpa_sotab = np.eye(n_values)[values]\n",
    "y_train_cpa_sotab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(test_table_sotab_cpa['encoded_label'])\n",
    "n_values = np.max(values) + 1\n",
    "y_test_cpa_sotab = np.eye(n_values)[values]\n",
    "y_test_cpa_sotab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "chi_square = SelectKBest(chi2, k=5000)\n",
    "X_train_cpa_sotab = chi_square.fit_transform(X_train_cpa_sotab, y_train_cpa_sotab)\n",
    "X_test_cpa_sotab = chi_square.transform(X_test_cpa_sotab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(\"Number of features:\", len(feature_names))\n",
    "print(\"Some example features:\", feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = chi_square.get_feature_names_out()\n",
    "print(\"Number of features:\", len(feature_names))\n",
    "print(\"Some example features:\", feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509, 5000)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cpa_sotab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109994, 5000)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cpa_sotab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cpa_sotab = CatBoostClassifier(\n",
    "    loss_function='MultiLogloss',\n",
    "    learning_rate=0.2,\n",
    "    iterations=100\n",
    ")\n",
    "model_cpa_sotab.fit(X_train_cpa_sotab, y_train_cpa_sotab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cpa_sotab = model_cpa_sotab.predict(X_test_cpa_sotab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.22      0.27         9\n",
      "           1       1.00      0.11      0.20         9\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.75      1.00      0.86         6\n",
      "           5       0.00      0.00      0.00         7\n",
      "           6       0.00      0.00      0.00        12\n",
      "           7       1.00      0.50      0.67         4\n",
      "           8       0.75      0.60      0.67         5\n",
      "           9       0.00      0.00      0.00         7\n",
      "          10       0.58      0.58      0.58        12\n",
      "          11       1.00      0.25      0.40         8\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      0.43      0.60         7\n",
      "          14       1.00      0.62      0.77         8\n",
      "          15       1.00      0.88      0.93         8\n",
      "          16       1.00      0.17      0.29         6\n",
      "          17       1.00      0.44      0.62         9\n",
      "          18       1.00      0.50      0.67         6\n",
      "          19       0.00      0.00      0.00         2\n",
      "          20       0.00      0.00      0.00         5\n",
      "          21       0.00      0.00      0.00         2\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       1.00      0.10      0.18        10\n",
      "          24       0.00      0.00      0.00         7\n",
      "          25       0.00      0.00      0.00         6\n",
      "          26       0.00      0.00      0.00         5\n",
      "          27       0.00      0.00      0.00         5\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      0.75      0.86         4\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         7\n",
      "          32       0.00      0.00      0.00         7\n",
      "          33       0.00      0.00      0.00         2\n",
      "          34       0.00      0.00      0.00         9\n",
      "          35       1.00      0.09      0.17        11\n",
      "          36       1.00      0.60      0.75         5\n",
      "          37       1.00      0.33      0.50         6\n",
      "          38       0.00      0.00      0.00         5\n",
      "          39       0.00      0.00      0.00         8\n",
      "          40       1.00      0.20      0.33         5\n",
      "          41       0.00      0.00      0.00         2\n",
      "          42       0.00      0.00      0.00         6\n",
      "          43       0.50      0.50      0.50         6\n",
      "          44       0.00      0.00      0.00         4\n",
      "          45       0.00      0.00      0.00         2\n",
      "          46       1.00      0.50      0.67         2\n",
      "          47       0.00      0.00      0.00         2\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       1.00      0.50      0.67         4\n",
      "          50       0.00      0.00      0.00         6\n",
      "          51       0.00      0.00      0.00         3\n",
      "          52       0.00      0.00      0.00         5\n",
      "          53       0.00      0.00      0.00         3\n",
      "          54       0.00      0.00      0.00        11\n",
      "          55       1.00      0.40      0.57         5\n",
      "          56       0.00      0.00      0.00        10\n",
      "          57       0.00      0.00      0.00         2\n",
      "          58       0.00      0.00      0.00         4\n",
      "          59       0.00      0.00      0.00         4\n",
      "          60       0.00      0.00      0.00         6\n",
      "          61       0.00      0.00      0.00        10\n",
      "          62       0.00      0.00      0.00         4\n",
      "          63       0.00      0.00      0.00         5\n",
      "          64       0.00      0.00      0.00         7\n",
      "          65       0.00      0.00      0.00         2\n",
      "          66       0.00      0.00      0.00         2\n",
      "          67       0.00      0.00      0.00         2\n",
      "          68       0.00      0.00      0.00         3\n",
      "          69       1.00      0.43      0.60         7\n",
      "          70       0.00      0.00      0.00         4\n",
      "          71       0.00      0.00      0.00         4\n",
      "          72       1.00      0.43      0.60         7\n",
      "          73       1.00      0.25      0.40         4\n",
      "          74       0.00      0.00      0.00         4\n",
      "          75       0.00      0.00      0.00         6\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       0.00      0.00      0.00         4\n",
      "          78       0.00      0.00      0.00         3\n",
      "          79       0.00      0.00      0.00         3\n",
      "          80       1.00      1.00      1.00         2\n",
      "          81       0.00      0.00      0.00         3\n",
      "          82       0.00      0.00      0.00         3\n",
      "          83       1.00      0.50      0.67         2\n",
      "          84       0.00      0.00      0.00         1\n",
      "          85       0.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       0.00      0.00      0.00         2\n",
      "          88       0.00      0.00      0.00         3\n",
      "          89       0.00      0.00      0.00         2\n",
      "          90       0.00      0.00      0.00         2\n",
      "          91       1.00      1.00      1.00         2\n",
      "          92       1.00      0.25      0.40         4\n",
      "          93       1.00      0.75      0.86         4\n",
      "          94       1.00      0.67      0.80         3\n",
      "          95       0.00      0.00      0.00         5\n",
      "          96       1.00      0.33      0.50         3\n",
      "          97       0.00      0.00      0.00         3\n",
      "          98       0.40      0.22      0.29         9\n",
      "          99       1.00      0.50      0.67         4\n",
      "         100       1.00      0.50      0.67         4\n",
      "         101       0.00      0.00      0.00         3\n",
      "         102       1.00      1.00      1.00         2\n",
      "         103       0.00      0.00      0.00         2\n",
      "         104       0.00      0.00      0.00         2\n",
      "         105       0.00      0.00      0.00         2\n",
      "         106       0.00      0.00      0.00         3\n",
      "         107       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.82      0.19      0.31       509\n",
      "   macro avg       0.33      0.18      0.21       509\n",
      "weighted avg       0.38      0.19      0.23       509\n",
      " samples avg       0.19      0.19      0.19       509\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_cpa_sotab, y_pred_cpa_sotab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "precision, recall, macro, _ = precision_recall_fscore_support(y_test_cpa_sotab, y_pred_cpa_sotab, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_f1 = f1_score(y_test_cpa_sotab, y_pred_cpa_sotab, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CatBoost Evaluation model/sotab-CPA-model.pkl', 'wb') as file:\n",
    "    pickle.dump(model_cpa_sotab, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WikiTable CTA MULTI-LABEL CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from catboost.utils import eval_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "cta_train = pd.read_json('WikiData/train.table_col_type.json')\n",
    "cta_test = pd.read_json('WikiData/test.table_col_type.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "cta_all_labels = ['soccer.football_league', 'government.government_office_or_title', 'organization.non_profit_organization', 'olympics.olympic_games', 'cvg.cvg_genre', 'ice_hockey.hockey_position', 'tv.tv_network', 'aviation.airline', 'american_football.football_conference', 'soccer.football_world_cup', 'american_football.football_coach', 'military.military_unit', 'military.military_post', 'music.media_format', 'tv.tv_personality', 'baseball.baseball_team', 'cvg.cvg_developer', 'soccer.football_award', 'ice_hockey.hockey_team', 'tv.tv_writer', 'meteorology.tropical_cyclone_season', 'soccer.fifa', 'cvg.cvg_publisher', 'baseball.baseball_player', 'sports.sports_championship', 'soccer.football_team_manager', 'sports.golfer', 'baseball.baseball_position', 'military.rank', 'cvg.cvg_platform', 'music.musical_group', 'amusement_parks.ride', 'music.genre', 'music.lyricist', 'music.record_label', 'meteorology.tropical_cyclone', 'aviation.airport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_table=pd.DataFrame()\n",
    "all_values=[]\n",
    "labelsx=[]\n",
    "for index_cta, row_cta in cta_test.iterrows():\n",
    "    \n",
    "    for table_index,table_row in enumerate(row_cta[6]):\n",
    "        labels=[]\n",
    "        for label in  row_cta[7][table_index]:\n",
    "            if label in cta_all_labels:\n",
    "                labels.append(label)\n",
    "        \n",
    "\n",
    "        if len(labels)>0:\n",
    "            value_list=[]\n",
    "            for value_index, value in table_row:\n",
    "                value_list.append(value[1])\n",
    "            joined_value_list = ', '.join(map(str, value_list))   \n",
    "            \n",
    "            all_values.append(joined_value_list)\n",
    "\n",
    "            joined_label=labels\n",
    "            labelsx.append(joined_label)\n",
    "\n",
    "final_test_table['data']=all_values\n",
    "final_test_table['Target']=labelsx\n",
    "\n",
    "print(len(all_values))\n",
    "print(len(labelsx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_table=pd.DataFrame()\n",
    "all_values=[]\n",
    "labelsx=[]\n",
    "for index_cta, row_cta in cta_train.iterrows():\n",
    "    \n",
    "    for table_index,table_row in enumerate(row_cta[6]):\n",
    "        labels=[]\n",
    "        for label in  row_cta[7][table_index]:\n",
    "            if label in cta_all_labels:\n",
    "                labels.append(label)\n",
    "\n",
    "        if len(labels)>0:\n",
    "            value_list=[]\n",
    "            for value_index, value in table_row:\n",
    "                value_list.append(value[1])\n",
    "            joined_value_list = ', '.join(map(str, value_list))   \n",
    "            \n",
    "            all_values.append(joined_value_list)\n",
    "            joined_label=labels\n",
    "            labelsx.append(joined_label)\n",
    "\n",
    "final_train_table['data']=all_values\n",
    "final_train_table['Target']=labelsx\n",
    "\n",
    "print(len(all_values))\n",
    "print(len(labelsx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(807, 19252)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3)\n",
    "X_train = tfidf.fit_transform(final_train_table['data'])\n",
    "X_test = tfidf.transform(final_test_table['data'])\n",
    "\n",
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "types =[]\n",
    "for value in final_train_table['Target']:\n",
    "    for i in value:\n",
    "        types.append(i)\n",
    "types=list(set(types))\n",
    "\n",
    "indexes=[]\n",
    "for value in final_train_table['Target']:\n",
    "    cell=[]\n",
    "    for i in value:\n",
    "        cell.append(types.index(i))\n",
    "    indexes.append(cell)\n",
    "\n",
    "final_train_table['encoded_label']=indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesx = list(final_train_table['encoded_label'])\n",
    "find_max=[]\n",
    "for x in valuesx:\n",
    "    for i in x:\n",
    "        find_max.append(i)\n",
    "n_values = np.max(find_max) + 1\n",
    "target_encoded=[]\n",
    "for values in valuesx:\n",
    "    y_train = [0] * n_values\n",
    "    for i in values:\n",
    "        y_train =y_train+ np.eye(n_values)[i]\n",
    "    target_encoded.append(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = target_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_table['encoded_matrix']=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=[]\n",
    "for value in final_test_table['Target']:\n",
    "    cell=[]\n",
    "    for i in value:\n",
    "        cell.append(types.index(i))\n",
    "    indexes.append(cell)\n",
    "final_test_table['encoded_label']=indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesx = list(final_test_table['encoded_label'])\n",
    "target_encoded=[]\n",
    "for values in valuesx:\n",
    "    y_test = [0] * n_values\n",
    "    for i in values:\n",
    "        y_test =y_test+ np.eye(n_values)[i]\n",
    "    target_encoded.append(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = target_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_table['encoded_matrix']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>Target</th>\n",
       "      <th>encoded_label</th>\n",
       "      <th>encoded_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Premier League, Premier League, League One, Ch...</td>\n",
       "      <td>[soccer.football_league]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uni-President Lions, Wei Chuan Dragons, Uni-Pr...</td>\n",
       "      <td>[baseball.baseball_team]</td>\n",
       "      <td>[21]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wei Chuan Dragons, Sinon Bulls, Sinon Bulls, U...</td>\n",
       "      <td>[baseball.baseball_team]</td>\n",
       "      <td>[21]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Football, Football, Football, Ice hockey, Ice ...</td>\n",
       "      <td>[cvg.cvg_genre]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joel Skinner, Rick Rodriguez, Gerald Perry</td>\n",
       "      <td>[baseball.baseball_player]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>J. League Division 2, J. League Division 2, J....</td>\n",
       "      <td>[soccer.football_league]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>J. League Division 1, J. League Division 1, J....</td>\n",
       "      <td>[soccer.football_league]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>J. League Division 1, J. League Division 2, J....</td>\n",
       "      <td>[soccer.football_league]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Las Vegas 51s, Jacksonville Suns, Vero Beach D...</td>\n",
       "      <td>[baseball.baseball_team]</td>\n",
       "      <td>[21]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>Parlophone, Parlophone, Columbia, Parlophone, ...</td>\n",
       "      <td>[music.record_label]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>807 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  data  \\\n",
       "0    Premier League, Premier League, League One, Ch...   \n",
       "1    Uni-President Lions, Wei Chuan Dragons, Uni-Pr...   \n",
       "2    Wei Chuan Dragons, Sinon Bulls, Sinon Bulls, U...   \n",
       "3    Football, Football, Football, Ice hockey, Ice ...   \n",
       "4           Joel Skinner, Rick Rodriguez, Gerald Perry   \n",
       "..                                                 ...   \n",
       "802  J. League Division 2, J. League Division 2, J....   \n",
       "803  J. League Division 1, J. League Division 1, J....   \n",
       "804  J. League Division 1, J. League Division 2, J....   \n",
       "805  Las Vegas 51s, Jacksonville Suns, Vero Beach D...   \n",
       "806  Parlophone, Parlophone, Columbia, Parlophone, ...   \n",
       "\n",
       "                         Target encoded_label  \\\n",
       "0      [soccer.football_league]          [23]   \n",
       "1      [baseball.baseball_team]          [21]   \n",
       "2      [baseball.baseball_team]          [21]   \n",
       "3               [cvg.cvg_genre]           [8]   \n",
       "4    [baseball.baseball_player]           [0]   \n",
       "..                          ...           ...   \n",
       "802    [soccer.football_league]          [23]   \n",
       "803    [soccer.football_league]          [23]   \n",
       "804    [soccer.football_league]          [23]   \n",
       "805    [baseball.baseball_team]          [21]   \n",
       "806        [music.record_label]          [11]   \n",
       "\n",
       "                                        encoded_matrix  \n",
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
       "4    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "..                                                 ...  \n",
       "802  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "803  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "804  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "805  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "806  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[807 rows x 4 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>Target</th>\n",
       "      <th>encoded_label</th>\n",
       "      <th>encoded_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Persian Gulf Cup, Persian Gulf Cup, Azadegan L...</td>\n",
       "      <td>[soccer.football_league]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pro League, Pro League, Pro League, Pro League...</td>\n",
       "      <td>[soccer.football_league]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Villone, Bob Wolcott, Chris Widger, Andy S...</td>\n",
       "      <td>[baseball.baseball_player]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ligat ha'Al, Ligat ha'Al, Ligat ha'Al, Liga Le...</td>\n",
       "      <td>[soccer.football_league]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nine Network, ARD, RTL plus, RCTI, RCTI, RCTI,...</td>\n",
       "      <td>[tv.tv_network]</td>\n",
       "      <td>[30]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38715</th>\n",
       "      <td>Peter Gwinn and Rob Dubbin, Peter Gwinn and La...</td>\n",
       "      <td>[tv.tv_writer]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38716</th>\n",
       "      <td>Argentine Primera, Argentine Primera, Argentin...</td>\n",
       "      <td>[soccer.football_league]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38717</th>\n",
       "      <td>Seven Network, Dubai TV, ZDF, TVN, NBC</td>\n",
       "      <td>[tv.tv_network]</td>\n",
       "      <td>[30]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38718</th>\n",
       "      <td>737 NAS, 800 NAS, 801 NAS, 809 NAS, 815 NAS, 8...</td>\n",
       "      <td>[military.military_unit]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38719</th>\n",
       "      <td>Hiroshima, Hiroshima, Hiroshima, Hiroshima, Hi...</td>\n",
       "      <td>[baseball.baseball_team]</td>\n",
       "      <td>[21]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38720 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    data  \\\n",
       "0      Persian Gulf Cup, Persian Gulf Cup, Azadegan L...   \n",
       "1      Pro League, Pro League, Pro League, Pro League...   \n",
       "2      Ron Villone, Bob Wolcott, Chris Widger, Andy S...   \n",
       "3      Ligat ha'Al, Ligat ha'Al, Ligat ha'Al, Liga Le...   \n",
       "4      Nine Network, ARD, RTL plus, RCTI, RCTI, RCTI,...   \n",
       "...                                                  ...   \n",
       "38715  Peter Gwinn and Rob Dubbin, Peter Gwinn and La...   \n",
       "38716  Argentine Primera, Argentine Primera, Argentin...   \n",
       "38717             Seven Network, Dubai TV, ZDF, TVN, NBC   \n",
       "38718  737 NAS, 800 NAS, 801 NAS, 809 NAS, 815 NAS, 8...   \n",
       "38719  Hiroshima, Hiroshima, Hiroshima, Hiroshima, Hi...   \n",
       "\n",
       "                           Target encoded_label  \\\n",
       "0        [soccer.football_league]          [23]   \n",
       "1        [soccer.football_league]          [23]   \n",
       "2      [baseball.baseball_player]           [0]   \n",
       "3        [soccer.football_league]          [23]   \n",
       "4                 [tv.tv_network]          [30]   \n",
       "...                           ...           ...   \n",
       "38715              [tv.tv_writer]           [5]   \n",
       "38716    [soccer.football_league]          [23]   \n",
       "38717             [tv.tv_network]          [30]   \n",
       "38718    [military.military_unit]          [10]   \n",
       "38719    [baseball.baseball_team]          [21]   \n",
       "\n",
       "                                          encoded_matrix  \n",
       "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                  ...  \n",
       "38715  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "38716  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "38717  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "38718  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "38719  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[38720 rows x 4 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CatBoostClassifier(\n",
    "    loss_function='MultiLogloss',\n",
    "    #eval_metric='HammingLoss',\n",
    "    iterations=100,\n",
    "    learning_rate=0.2\n",
    ")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_f1 = f1_score(y_test, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7370753323485968"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5476315873897606"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7117339543354456"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.461617358145046"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CatBoost Evaluation model/wikiTables-CTA-model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WikiTable CPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpa_train = pd.read_json('WikiData/train.table_rel_extraction.json')\n",
    "cpa_test = pd.read_json('WikiData/test.table_rel_extraction.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpa_all_labels = ['time.event.locations', 'music.artist.album', 'sports.sports_team.sport', 'baseball.baseball_team.league', 'tv.tv_program.country_of_origin', 'music.album.artist', 'sports.sports_team.location', 'time.event.instance_of_recurring_event', 'aviation.airline.hubs', 'sports.sports_championship_event.champion', 'sports.sports_facility.teams', 'baseball.baseball_player.position_s', 'sports.sports_league.teams-sports.sports_league_participation.team', 'tv.tv_network.programs-tv.tv_network_duration.program', 'sports.sports_league_season.league', 'olympics.olympic_athlete.country-olympics.olympic_athlete_affiliation.country', 'american_football.football_player.position_s', 'music.composer.compositions', 'meteorology.tropical_cyclone.tropical_cyclone_season', 'cvg.computer_videogame.developer', 'tv.tv_character.appeared_in_tv_program-tv.regular_tv_appearance.actor', 'cvg.computer_videogame.publisher', 'soccer.football_player.position_s', 'tv.tv_program.original_network-tv.tv_network_duration.network', 'music.composition.composer', 'ice_hockey.hockey_player.hockey_position', 'book.author.works_written', 'film.film.genre', 'film.film.directed_by', 'film.film.produced_by', 'film.film.language', 'broadcast.broadcast.area_served', 'award.award_category.category_of', 'location.location.nearby_airports', 'location.country.official_language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpa_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = []\n",
    "\n",
    "for primary_row_index, primary_row_values in cpa_test.iterrows():\n",
    "    new_col_labels = []\n",
    "    for col_index, col_labels in enumerate(primary_row_values[7]):\n",
    "        sampled_labels = []\n",
    "        for label in col_labels:\n",
    "            if label in cpa_all_labels:\n",
    "                sampled_labels.append(label)\n",
    "        \n",
    "        if len(sampled_labels) !=0:\n",
    "            new_col_labels.append(sampled_labels)\n",
    "\n",
    "            column_string = \"\"\n",
    "\n",
    "            for row_index in primary_row_values[6][0][:5]:\n",
    "                column_string += f\"{row_index[1][1]} \"\n",
    "            column_string = column_string.strip()\n",
    "\n",
    "            column_string += \"\"\n",
    "        \n",
    "            for row_index in primary_row_values[6][col_index+1][:5]:\n",
    "                column_string += f\"{row_index[1][1]} \"\n",
    "            column_string = column_string.strip()\n",
    "\n",
    "            test_examples.append([primary_row_values[0], col_index, column_string, sampled_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "\n",
    "for primary_row_index, primary_row_values in cpa_train.iterrows():\n",
    "    new_col_labels = []\n",
    "    for col_index, col_labels in enumerate(primary_row_values[7]):\n",
    "        sampled_labels = []\n",
    "        for label in col_labels:\n",
    "            if label in cpa_all_labels:\n",
    "                sampled_labels.append(label)\n",
    "        \n",
    "        if len(sampled_labels) !=0:\n",
    "            new_col_labels.append(sampled_labels)\n",
    "\n",
    "            column_string = \"\"\n",
    "\n",
    "            for row_index in primary_row_values[6][0][:5]:\n",
    "                column_string += f\"{row_index[1][1]} \"\n",
    "            column_string = column_string.strip()\n",
    "\n",
    "            column_string += \"\"\n",
    "        \n",
    "            for row_index in primary_row_values[6][col_index+1][:5]:\n",
    "                column_string += f\"{row_index[1][1]} \"\n",
    "            column_string = column_string.strip()\n",
    "\n",
    "            train_examples.append([primary_row_values[0], col_index, column_string, sampled_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table_cpa=pd.DataFrame()\n",
    "labels=[]\n",
    "values=[]\n",
    "for i in test_examples:\n",
    "    labels.append(i[-1])\n",
    "    values.append(i[2])\n",
    "\n",
    "final_table_cpa['data']=values\n",
    "final_table_cpa['cpa label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table_cpa=pd.DataFrame()\n",
    "labels=[]\n",
    "values=[]\n",
    "for i in train_examples:\n",
    "    labels.append(i[-1])\n",
    "    values.append(i[2])\n",
    "\n",
    "final_table_cpa['data']=values\n",
    "final_table_cpa['cpa label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table_cpa=final_table_cpa\n",
    "train_table_cpa=train_table_cpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_cpa=final_table_cpa\n",
    "test_table_cpa=test_table_cpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3)\n",
    "X_train_cpa = tfidf.fit_transform(train_table_cpa['data'])\n",
    "X_test_cpa = tfidf.transform(test_table_cpa['data'])\n",
    "\n",
    "X_test_cpa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cpa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "types =[]\n",
    "for value in train_table_cpa['cpa label']:\n",
    "    for i in value:\n",
    "        types.append(i)\n",
    "types=list(set(types))\n",
    "\n",
    "indexes=[]\n",
    "for value in train_table_cpa['cpa label']:\n",
    "    cell=[]\n",
    "    for i in value:\n",
    "        cell.append(types.index(i))\n",
    "    indexes.append(cell)\n",
    "\n",
    "train_table_cpa['encoded_label']=indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesx = list(train_table_cpa['encoded_label'])\n",
    "\n",
    "find_max=[]\n",
    "for x in valuesx:\n",
    "    for i in x:\n",
    "        find_max.append(i)\n",
    "\n",
    "n_values = np.max(find_max) + 1\n",
    "\n",
    "target_encoded=[]\n",
    "for values in valuesx:\n",
    "    y_train = [0] * n_values\n",
    "    for i in values:\n",
    "\n",
    "        y_train =y_train+ np.eye(n_values)[i]\n",
    "    target_encoded.append(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cpa=target_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=[]\n",
    "for value in test_table_cpa['cpa label']:\n",
    "    cell=[]\n",
    "    for i in value:\n",
    "        cell.append(types.index(i))\n",
    "    indexes.append(cell)\n",
    "\n",
    "test_table_cpa['encoded_label']=indexes\n",
    "\n",
    "valuesx = list(test_table_cpa['encoded_label'])\n",
    "\n",
    "target_encoded=[]\n",
    "for values in valuesx:\n",
    "    y_test = [0] * n_values\n",
    "    for i in values:\n",
    "\n",
    "        y_test =y_test+ np.eye(n_values)[i]\n",
    "    target_encoded.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cpa=target_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cpa = CatBoostClassifier(\n",
    "    loss_function='MultiLogloss',\n",
    "    #eval_metric='HammingLoss',\n",
    "    iterations=100,\n",
    "    learning_rate=0.2\n",
    ")\n",
    "clf_cpa.fit(X_train_cpa, y_train_cpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cpa = clf_cpa.predict(X_test_cpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7214953271028037\n"
     ]
    }
   ],
   "source": [
    "accuracy = eval_metric(y_test_cpa, y_pred_cpa, 'Accuracy')[0]\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_class = eval_metric(y_test_cpa, y_pred_cpa, 'Accuracy:type=PerClass')\n",
    "for cls, value in zip(clf.classes_, accuracy_per_class):\n",
    "    print(f'Accuracy for class {cls}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ('Precision', 'Recall', 'F1'):\n",
    "    print(metric)\n",
    "    values = eval_metric(y_test_cpa, y_pred_cpa, metric)\n",
    "    for cls, value in zip(clf.classes_, values):\n",
    "        print(f'class={cls}: {value:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_cpa, y_pred_cpa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, macro, _ = precision_recall_fscore_support(y_test_cpa, y_pred_cpa, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_f1 = f1_score(y_test_cpa, y_pred_cpa, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5917938311691"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8385026737967914"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5024201991250914"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7922813569872393"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('CatBoost Evaluation model/wikiTables-CPA-model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf_cpa, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
